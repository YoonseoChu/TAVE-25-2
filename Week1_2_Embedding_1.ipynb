{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **(파이토치 트랜스포머를 활용한) 자연어 처리와 컴퓨터비전 심층학습**\n",
        "## 6장 토큰화 (p264~ p298)"
      ],
      "metadata": {
        "id": "QtuqA8n3Aa8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [NLTK 라이브러리로 구현한 N-gram]"
      ],
      "metadata": {
        "id": "nwdLix5WP5b0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "def ngrams(sentence, n):\n",
        "    words = sentence.split()\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    return list(ngrams)\n",
        "\n",
        "sentence = \"안녕하세요 만나서 진심으로 반가워요\"\n",
        "\n",
        "unigram = ngrams(sentence, 1)\n",
        "bigram = ngrams(sentence, 2)\n",
        "trigram = ngrams(sentence, 3)\n",
        "\n",
        "print(unigram)\n",
        "print(bigram)\n",
        "print(trigram)\n",
        "\n",
        "unigram = nltk.ngrams(sentence.split(), 1)\n",
        "bigram = nltk.ngrams(sentence.split(), 2)\n",
        "trigram = nltk.ngrams(sentence.split(), 3)\n",
        "\n",
        "print(list(unigram))\n",
        "print(list(bigram))\n",
        "print(list(trigram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qblkue6hP9AU",
        "outputId": "f3115f6c-b23c-4d14-dc13-988ab3806adc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n",
            "[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n",
            "[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n",
            "[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n",
            "[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n",
            "[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [TF-IDF 계산]"
      ],
      "metadata": {
        "id": "uCsku827QCuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    \"That movie is famous movie\",\n",
        "    \"I like that actor\",\n",
        "    \"I don’t like that actor\"\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(corpus)\n",
        "tfidf_matrix = tfidf_vectorizer.transform(corpus)\n",
        "# tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(tfidf_matrix.toarray())\n",
        "print(tfidf_vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw-88bYMUJ5d",
        "outputId": "b01146e3-0fbc-4fba-8b24-4eb5ff609730"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.39687454 0.39687454 0.         0.79374908\n",
            "  0.2344005 ]\n",
            " [0.61980538 0.         0.         0.         0.61980538 0.\n",
            "  0.48133417]\n",
            " [0.4804584  0.63174505 0.         0.         0.4804584  0.\n",
            "  0.37311881]]\n",
            "{'that': 6, 'movie': 5, 'is': 3, 'famous': 2, 'like': 4, 'actor': 0, 'don': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Skip-gram 모델 실습]\n",
        "영화 리뷰 데이터세트"
      ],
      "metadata": {
        "id": "ZPAgl1HIUKJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece Korpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "6A-yli8KBB6i",
        "outputId": "d46018a9-7d79-460a-c2f8-17fcdb39434c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Collecting Korpora\n",
            "  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting dataclasses>=0.6 (from Korpora)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from Korpora) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.12/dist-packages (from Korpora) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from Korpora) (2.32.4)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from Korpora) (2.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20.0->Korpora) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20.0->Korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20.0->Korpora) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20.0->Korpora) (2025.8.3)\n",
            "Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: dataclasses, Korpora\n",
            "Successfully installed Korpora-0.2.0 dataclasses-0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              },
              "id": "6aaffcf40de747e2813aa394a47ebe63"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw85MU_5WgJr",
        "outputId": "9dac8a56-973f-4c88-ffb6-600d64917bb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.9/495.9 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.6.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 Skip-gram 클래스\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class VanillaSkipgram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim\n",
        "        )\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=embedding_dim,\n",
        "            out_features=vocab_size\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.embedding(input_ids)\n",
        "        output = self.linear(embeddings)\n",
        "        return output"
      ],
      "metadata": {
        "id": "uJBxaTbXV8rT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 영화 리뷰 데이터세트 전처리\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEB6w8IwV-tX",
        "outputId": "2d648b11-3313-4522-fec9-66978d786205"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nsmc] download ratings_train.txt: 14.6MB [00:00, 20.0MB/s]                            \n",
            "[nsmc] download ratings_test.txt: 4.90MB [00:00, 13.4MB/s]                            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]\n",
        "print(tokens[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGUM7vT2WLNX",
        "outputId": "3a04dcc0-859a-4c3c-9258-e9ab00232ddb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 사전 구축\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpEGM3wcWBFB",
        "outputId": "74d9d438-d010-4628-b544-ec98065c3448"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '.', '이', '영화', '의', '..', '가', '에', '...', '을']\n",
            "5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip-gram의 단어 쌍 추출\n",
        "def get_word_pairs(tokens, window_size):\n",
        "    pairs = []\n",
        "    for sentence in tokens:\n",
        "        sentence_length = len(sentence)\n",
        "        for idx, center_word in enumerate(sentence):\n",
        "            window_start = max(0, idx - window_size)\n",
        "            window_end = min(sentence_length, idx + window_size + 1)\n",
        "            center_word = sentence[idx]\n",
        "            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n",
        "            for context_word in context_words:\n",
        "                pairs.append([center_word, context_word])\n",
        "    return pairs\n",
        "\n",
        "\n",
        "word_pairs = get_word_pairs(tokens, window_size=2)\n",
        "print(word_pairs[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnouygZeWN_-",
        "outputId": "e268139d-103b-486c-9d43-d719d07e085d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['굳', 'ㅋ'], ['ㅋ', '굳'], ['뭐', '야'], ['뭐', '이'], ['야', '뭐']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 인덱스 쌍 변환\n",
        "def get_index_pairs(word_pairs, token_to_id):\n",
        "    pairs = []\n",
        "    unk_index = token_to_id[\"<unk>\"]\n",
        "    for word_pair in word_pairs:\n",
        "        center_word, context_word = word_pair\n",
        "        center_index = token_to_id.get(center_word, unk_index)\n",
        "        context_index = token_to_id.get(context_word, unk_index)\n",
        "        pairs.append([center_index, context_index])\n",
        "    return pairs\n",
        "\n",
        "\n",
        "index_pairs = get_index_pairs(word_pairs, token_to_id)\n",
        "print(index_pairs[:5])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lx7f5ZRWPlS",
        "outputId": "c9bc7410-a2c6-45a2-9027-3dedf5427cb8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[595, 100], [100, 595], [77, 176], [77, 2], [176, 77]]\n",
            "5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터로더 적용\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "index_pairs = torch.tensor(index_pairs)\n",
        "center_indexes = index_pairs[:, 0]\n",
        "context_indexes = index_pairs[:, 1]\n",
        "\n",
        "dataset = TensorDataset(center_indexes, context_indexes)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "tfFT-cLaWRCb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip-gram 모델 준비 작업\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "word2vec = VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "41Nuprm3WSZO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "# 학습시간이 너무 길어서 생략\n",
        "\n",
        "for epoch in range(10):\n",
        "    cost = 0.0\n",
        "    for input_ids, target_ids in dataloader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        logits = word2vec(input_ids)\n",
        "        loss = criterion(logits, target_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cost += loss\n",
        "\n",
        "    cost = cost / len(dataloader)\n",
        "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"
      ],
      "metadata": {
        "id": "PegYTgPnYVq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 값 추출\n",
        "token_to_embedding = dict()\n",
        "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, embedding in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word] = embedding\n",
        "\n",
        "index = 30\n",
        "token = vocab[index]\n",
        "token_embedding = token_to_embedding[token]\n",
        "print(token)\n",
        "print(token_embedding)"
      ],
      "metadata": {
        "id": "Ua7q37m9WWJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 임베딩 유사도 계산\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))\n",
        "    return cosine\n",
        "\n",
        "def top_n_index(cosine_matrix, n):\n",
        "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
        "    top_n = closest_indexes[1 : n + 1]\n",
        "    return top_n\n",
        "\n",
        "\n",
        "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
        "top_n = top_n_index(cosine_matrix, n=5)\n",
        "\n",
        "print(f\"{token}와 가장 유사한 5 개 단어\")\n",
        "for index in top_n:\n",
        "    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")"
      ],
      "metadata": {
        "id": "MStzufNYWXz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Word2Vec 모델 학습]"
      ],
      "metadata": {
        "id": "CvkqwKyrWZPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "-o1xtk7OYnB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec 모델 학습\n",
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPK8BhnoXNFa",
        "outputId": "d58931d0-1ec5-4abb-c96b-8708531d29f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nsmc] download ratings_train.txt: 14.6MB [00:00, 84.4MB/s]                           \n",
            "[nsmc] download ratings_test.txt: 4.90MB [00:00, 33.5MB/s]                           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]"
      ],
      "metadata": {
        "id": "VRzxPhmjY__S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "word2vec = Word2Vec(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1,\n",
        "    epochs=3,\n",
        "    max_final_vocab=10000\n",
        ")"
      ],
      "metadata": {
        "id": "VBLxlY-5XY_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.save(\"./word2vec.model\")\n",
        "word2vec = Word2Vec.load(\"./word2vec.model\")"
      ],
      "metadata": {
        "id": "gs3_qr-WXe1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 임베딩 추출 및 유사도 계산\n",
        "word = \"연기\"\n",
        "print(word2vec.wv[word])\n",
        "print(word2vec.wv.most_similar(word, topn=5))\n",
        "print(word2vec.wv.similarity(w1=word, w2=\"연기력\"))"
      ],
      "metadata": {
        "id": "8SEXu0akXgqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}